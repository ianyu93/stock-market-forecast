{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Monthly Predictor Training**\n",
    "## December 2020\n",
    "### Ian Yu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Table of Content**\n",
    "\n",
    "1. [Objective](#Objective)\n",
    "2. [Setup](#Setup)\n",
    "3. [Preprocessing](#Preprocessing)\n",
    "4. [Model Building](#Model-Building)\n",
    "5. [Model Training](#Model-Training)\n",
    "6. [Model Summary](#Model-Summary)\n",
    "6. [Next Step](#Next-Step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Objective**\n",
    "\n",
    "The purpose of this notebook is to take the parameters provided in the previous notebook, '3-Hypertuning with 20-Day Engineered Dataframe', and train the model. We once again split the model training into 3 different notebooks as we perform the training process simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setup**\n",
    "\n",
    "As discussed in the previous notebook, we will be training a Bidirectional Long Short-Term Memory Recurrent Neural Network. Unlike the previous notebook, however, since we already performed hypertuning, we will just be training the entire train set, which is all dates before 2019-08-18, and find the best model based on lowest loss value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing boto3 to call object from AWS S3 personal bucket\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Object('capstone-ianyu', '2-20dengineered_df.csv').download_file('data/2-20dengineered_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: The ZIP file includes a data folder with the dataset, so one could simply run the cell below instead.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing basic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Read the clean dataset\n",
    "df = pd.read_csv(\"data/2-20dengineered_df.csv\", index_col = 0)\n",
    "\n",
    "# To ensure the index is set at datetime\n",
    "df.index = pd.to_datetime(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preprocessing**\n",
    "\n",
    "We're going to split the train test and test set based on the notion that we will make prediction continouously for 250 days. After splitting, we will define the target, `SPX Today`, and features for both the train set and the test set. We will also be using MinMaxScaler, a more common scaler for time series problems, to scale our features in train set and test set. Note that MinMaxScaler would scale the features so that different features would stay on the same scale. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test Split, continously predicting for 250 days\n",
    "train = df[:-250]\n",
    "test = df[-250:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting features and target for train set\n",
    "X_train = train.drop('SPX Today', axis = 1)\n",
    "y_train = train['SPX Today']\n",
    "\n",
    "# Setting features and target for test set\n",
    "X_test = test.drop('SPX Today', axis = 1)\n",
    "y_test = test['SPX Today']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting MinMax Scaler onto the remainder set\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mmscaler = MinMaxScaler()\n",
    "\n",
    "# Converting to Dataframe to keep the Timestamps\n",
    "X_train_mms = pd.DataFrame(data = mmscaler.fit_transform(X_train), columns = X_train.columns, index = train.index)\n",
    "X_test_mms = pd.DataFrame(data = mmscaler.transform(X_test), columns = X_test.columns, index = test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping features and target in order to fit through our model\n",
    "Xtrain = np.array(X_train_mms).reshape(np.array(X_train_mms).shape[0],-1,X_train_mms.shape[1])\n",
    "ytrain = np.array(y_train).reshape(np.array(y_train).shape[0],-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Building**\n",
    "\n",
    "The architecture that we will be building is comprised of four elements. It will be a single layer of Bidirectional Long Short-Term Memory Recurrent Neural Network with an output layer of Time Distributed Layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recurrent Neural Netowrk** is a type of neural network that allows us to learn backwards in sequence. It takes in a 3D shape array, input the data as (batch size, sequence, features). Batch size would be the number of data points that we are passing into the neural network at once. If we have a batch size of 1024, then we are passing 1024 trading days into the network at a time. We will have the hypertuner to determine what is the optimal batch size. Sequence length is about the past context, where if we set a sequence length of 10, the model would learn how does the previous 10 sequence affect the current input. We will also leave it to the hypertuner to determine the best sequence length parameter. Features dimension would simply be the number of features we have in our dataset.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Recurrent_neural_network_unfold.svg/2880px-Recurrent_neural_network_unfold.svg.png\" height=800 width=800>\n",
    "\n",
    "*image from [Wikepedia](https://en.wikipedia.org/wiki/Recurrent_neural_network#/media/File:Recurrent_neural_network_unfold.svg)*\n",
    "\n",
    "**Long Short-Term Memory** is a special type of RNN that learns about the long term default behaviour of the dataset. In effect, this would decompose seasonality, trends, and other potential long-term patterns. \n",
    "\n",
    "**Bidirectional** is applied to the LSTM RNN for the purpose of learning the future context as well. Not only does the past affect the stock market today, but also the anticipation of tomorrow's environment would affect today's market. Therefore, we would also need to understand how that anticipation affects today's price. The bidirectional element creates a separate network in the same training session that learns forwards in the sequence instead, so that each time the network is learning both backwards and forward in time. \n",
    "\n",
    "**Time Distributed Layer** is a special type of output layer that keeps the training input and output one at a time, keeping the timestamps true. Without the layer, the default behaviour of RNN would learn and output in batches instead. \n",
    "\n",
    "*Note 1*: The inclusion of both Bidirectional element and the Time Distributed Layer was inspired by [Solving Sequence Problems with LSTM in Keras: Part 2](https://stackabuse.com/solving-sequence-problems-with-lstm-in-keras-part-2/). \n",
    "\n",
    "*Note 2*: This architecture is the final result of many trials and errors through monitoring the loss and train/validation metrics. Experiments include trying much more complicated structure, such as including RepeatVector mentioned in the tutorial from *Note 1*. We did not find more complex layers performed any better than the current architecture. Part of the decision was also influenced by [Optimizing LSTM for time series prediction in Indian stock market](https://pdf.sciencedirectassets.com/280203/1-s2.0-S1877050920X00056/1-s2.0-S1877050920307237/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjECQaCXVzLWVhc3QtMSJIMEYCIQCaPKGIG7DdWGcWijUd%2BmhnqxFo0lkvbaeANVEVr4Gx8AIhAP%2Fxavemri255BxSxSlvzXZIJ1rP2SZ7no2NS4uWTJgDKr0DCK3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQAxoMMDU5MDAzNTQ2ODY1IgzpgbluX2qDaLCDNLoqkQOaVtZa17kPj7Fik6ElKg8ZfyV8AGB6Gh3QEEvPhW655oNfCFFaVA2hzgs3ytWBA4e7AaFcCQifbETiM8qSekN5YP02N6MiSD9EV5uuhV3RHTMM6I6De3WwITh6Vg2SwhhVFb%2Bdxf7XgXbK9g7%2Fok1ctc6MhHQ%2FJHzD7AwF7NTJm2mzV0H%2F4tenShk153gVsDqWnHLrW2wYjQg7LtBAy2H89G9gI7TK1WIECXkutt6Moxh1GO1nRQxGIryu5OGnHphCIdAz3oEj91X7loih1geTon9J5EB3HSNeb%2BM%2Fhk%2BfNCInWLgPBnOJdQrX9A9OpbVVcF%2BvacbiPmS4OyGd8%2BkHVLE2svGbkDyiKTokP0TPBCh9NB9S3Ovt%2BoJbx6yGzXdu1gvqREgnqYcezySe5BdxygSCLg1iVcJx%2FLTduJo%2BLLt7EYycHSg14SiVLAO%2F%2B2AQxtV3FfYZZqqAdcuAj6epR1pqyJ%2BF%2FWZp6xHlEXhR4tnscsDvDExku%2FEJI0%2B6DAIL1POcsxP%2Bb9FGqCPd5WlNTzDl35%2F%2BBTrqAS8O9L6e1THIH3VjiRI5bLGQ%2FRNXXoc9oNfpqCOVjANdDrSE2tThA9V%2BNKd14Qb57yciOn93apfbjYvYdovXogLJ0cBBx3N%2FI6y2Z6ZsW0aIEKHx%2Bz6T3e6G5aXLafqMP%2B4JBiUYx0KmS8n8A%2BbbswEqYtbHkGFJxbKSjEzFbT8%2FLaFVD9mQMwZLs55UYmrohOysGZk%2BWnJjtsFj9W9TjYCLIpsXIGQBHABPNbPsDbQ9%2BJYk%2F3lY3xRmA060j3%2F0lHqQf%2Fy3svPUe7xhtCNAMo%2FSWMoxgdWJTmW0wE1MLmRbSYSMJrAqZN4pZg%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20201202T210705Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY3ADR5CWC%2F20201202%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=8fe53ed1da02e8720b05dbd4c47599a135d812d489b217c1e155ea7f8eba50c3&hash=f41fa7bc182cb22c436f1de61016bf5ecaa6158ccab1236efb2e593a3f997dd4&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1877050920307237&tid=spdf-af325d5e-2008-49c1-9511-03d086ba6774&sid=a182baa1112258464a79b52-2c38f3e10bdfgxrqa&type=client), finding more LSTM layers does not increase performance, but does stabalize the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a nutshell, our model architecture looks like this:\n",
    "\n",
    "<img src=\"images/ModelArchitecture.png\" height=800 width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing tensorflow and keras pakcages needed\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Bidirectional,TimeDistributed\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be building the model based on the configuration from the previous notebook, which would be:\n",
    "\n",
    "|Parameters|Configuration|\n",
    "|:---|:---|\n",
    "|**LSTM Units**|192|\n",
    "|**Sequence Length**|30|\n",
    "|**Initial Learning Rate**|0.01|\n",
    "|**Decay Steps**|10,000|\n",
    "|**Clipnorm**|0.1|\n",
    "\n",
    "We will also be using Mean Squared Error as our loss function and Root Mean Squared Error as our human readable metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learning rate schedule code is from Tensorflow documentation:\n",
    "## https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9, staircase = True)\n",
    "\n",
    "\n",
    "## Modeling with sequential\n",
    "model = keras.Sequential()\n",
    "\n",
    "## Building Bidrectional LSTM RNN\n",
    "# Configuration from previous notebook\n",
    "model.add(Bidirectional(LSTM(units = 192, activation='relu',\n",
    "                             input_shape = (30,Xtrain.shape[1]),\n",
    "                             return_sequences=True)))\n",
    "\n",
    "# Time Distributed layer to keep the model time series\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "## Compiling model with Adam optimizer\n",
    "model.compile(optimizer = keras.optimizers.Adam(clipnorm=0.1, learning_rate = lr_schedule),\n",
    "              loss = 'mse', metrics = 'RootMeanSquaredError')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Training**\n",
    "\n",
    "Now that we have built the architecture of the model, we can start training. However, we cannot just blindly start training, especially when we do not have a validation dataset to check if we are overfitting. Therefore, we need to define an Early Stopping mechanism to prevent overfitting. We will be monitoring the loss function, and if the loss value is not improving for five times in a row, then the training will automatically stop. \n",
    "\n",
    "We will also call for a Model Checkpoint. Usually the purpose of the model check point is to find the check point with the best validation metrics performance, but here we are also using the Model Checkpoint to save the best performing model as '10 Day Lag.h5'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: root_mean_squared_error improved from inf to 357.24127, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 127621.3281 - root_mean_squared_error: 357.2413\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: root_mean_squared_error improved from 357.24127 to 60.73831, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 3689.1418 - root_mean_squared_error: 60.7383\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: root_mean_squared_error improved from 60.73831 to 57.85147, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 3346.7932 - root_mean_squared_error: 57.8515\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: root_mean_squared_error improved from 57.85147 to 55.11834, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 3038.0315 - root_mean_squared_error: 55.1183\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: root_mean_squared_error improved from 55.11834 to 53.91841, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 2907.1946 - root_mean_squared_error: 53.9184\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: root_mean_squared_error improved from 53.91841 to 53.28731, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 2839.5374 - root_mean_squared_error: 53.2873\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: root_mean_squared_error improved from 53.28731 to 51.81206, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 2684.4893 - root_mean_squared_error: 51.8121\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: root_mean_squared_error improved from 51.81206 to 51.64548, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 2667.2556 - root_mean_squared_error: 51.6455\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: root_mean_squared_error improved from 51.64548 to 50.26408, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 2526.4780 - root_mean_squared_error: 50.2641\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: root_mean_squared_error improved from 50.26408 to 49.74216, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 2474.2827 - root_mean_squared_error: 49.7422\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: root_mean_squared_error did not improve from 49.74216\n",
      "271/271 - 1s - loss: 2700.9878 - root_mean_squared_error: 51.9710\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: root_mean_squared_error improved from 49.74216 to 48.11419, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 2314.9749 - root_mean_squared_error: 48.1142\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: root_mean_squared_error did not improve from 48.11419\n",
      "271/271 - 1s - loss: 2434.9717 - root_mean_squared_error: 49.3454\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: root_mean_squared_error did not improve from 48.11419\n",
      "271/271 - 1s - loss: 2476.4258 - root_mean_squared_error: 49.7637\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: root_mean_squared_error did not improve from 48.11419\n",
      "271/271 - 1s - loss: 2359.0369 - root_mean_squared_error: 48.5699\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: root_mean_squared_error improved from 48.11419 to 47.96663, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 2300.7979 - root_mean_squared_error: 47.9666\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: root_mean_squared_error did not improve from 47.96663\n",
      "271/271 - 1s - loss: 2350.5969 - root_mean_squared_error: 48.4830\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: root_mean_squared_error improved from 47.96663 to 47.61176, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 2266.8792 - root_mean_squared_error: 47.6118\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: root_mean_squared_error improved from 47.61176 to 46.08921, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 2124.2148 - root_mean_squared_error: 46.0892\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 00020: root_mean_squared_error did not improve from 46.08921\n",
      "271/271 - 1s - loss: 2300.6067 - root_mean_squared_error: 47.9646\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 00021: root_mean_squared_error improved from 46.08921 to 45.99745, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 2115.7651 - root_mean_squared_error: 45.9974\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 00022: root_mean_squared_error did not improve from 45.99745\n",
      "271/271 - 1s - loss: 2134.0662 - root_mean_squared_error: 46.1960\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: root_mean_squared_error improved from 45.99745 to 45.72033, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 2090.3481 - root_mean_squared_error: 45.7203\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 00024: root_mean_squared_error improved from 45.72033 to 45.59449, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 2078.8579 - root_mean_squared_error: 45.5945\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 00025: root_mean_squared_error did not improve from 45.59449\n",
      "271/271 - 1s - loss: 2188.4177 - root_mean_squared_error: 46.7805\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 00026: root_mean_squared_error improved from 45.59449 to 45.27956, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 2050.2390 - root_mean_squared_error: 45.2796\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 00027: root_mean_squared_error improved from 45.27956 to 45.03437, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 2028.0947 - root_mean_squared_error: 45.0344\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 00028: root_mean_squared_error improved from 45.03437 to 44.90889, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 2016.8081 - root_mean_squared_error: 44.9089\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 00029: root_mean_squared_error improved from 44.90889 to 44.25422, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 1958.4364 - root_mean_squared_error: 44.2542\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 00030: root_mean_squared_error did not improve from 44.25422\n",
      "271/271 - 1s - loss: 2014.2080 - root_mean_squared_error: 44.8799\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 00031: root_mean_squared_error did not improve from 44.25422\n",
      "271/271 - 1s - loss: 1970.9979 - root_mean_squared_error: 44.3959\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 00032: root_mean_squared_error did not improve from 44.25422\n",
      "271/271 - 1s - loss: 1988.9531 - root_mean_squared_error: 44.5977\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 00033: root_mean_squared_error improved from 44.25422 to 43.59213, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 1900.2740 - root_mean_squared_error: 43.5921\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 00034: root_mean_squared_error improved from 43.59213 to 43.28209, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 1873.3392 - root_mean_squared_error: 43.2821\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 00035: root_mean_squared_error did not improve from 43.28209\n",
      "271/271 - 1s - loss: 1931.5698 - root_mean_squared_error: 43.9496\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 00036: root_mean_squared_error improved from 43.28209 to 43.25565, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 1871.0509 - root_mean_squared_error: 43.2556\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 00037: root_mean_squared_error did not improve from 43.25565\n",
      "271/271 - 1s - loss: 1909.4816 - root_mean_squared_error: 43.6976\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 00038: root_mean_squared_error did not improve from 43.25565\n",
      "271/271 - 1s - loss: 1880.1543 - root_mean_squared_error: 43.3607\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 00039: root_mean_squared_error improved from 43.25565 to 42.62366, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 1816.7769 - root_mean_squared_error: 42.6237\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 00040: root_mean_squared_error improved from 42.62366 to 41.41463, saving model to /Users/ianyu/capstone/4-models/20 Day Lag.h5\n",
      "271/271 - 1s - loss: 1715.1714 - root_mean_squared_error: 41.4146\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 00041: root_mean_squared_error did not improve from 41.41463\n",
      "271/271 - 1s - loss: 1921.3893 - root_mean_squared_error: 43.8337\n",
      "Epoch 42/100\n",
      "\n",
      "Epoch 00042: root_mean_squared_error did not improve from 41.41463\n",
      "271/271 - 1s - loss: 1794.5992 - root_mean_squared_error: 42.3627\n",
      "Epoch 43/100\n",
      "\n",
      "Epoch 00043: root_mean_squared_error did not improve from 41.41463\n",
      "271/271 - 1s - loss: 1784.6066 - root_mean_squared_error: 42.2446\n",
      "Epoch 44/100\n",
      "\n",
      "Epoch 00044: root_mean_squared_error did not improve from 41.41463\n",
      "271/271 - 1s - loss: 1809.2422 - root_mean_squared_error: 42.5352\n",
      "Epoch 45/100\n",
      "\n",
      "Epoch 00045: root_mean_squared_error did not improve from 41.41463\n",
      "271/271 - 1s - loss: 1720.0620 - root_mean_squared_error: 41.4736\n"
     ]
    }
   ],
   "source": [
    "# Define early stopping, monitoring loss function, stops if not improving for five times in a row\n",
    "es = EarlyStopping(monitor='loss', patience = 5, verbose=0, mode='auto')\n",
    "\n",
    "# Define model checkpoint to save the best model weights and to be used immediately\n",
    "mc = ModelCheckpoint(\"/Users/ianyu/capstone/4-models/20 Day Lag.h5\",\n",
    "                     monitor=\"root_mean_squared_error\",\n",
    "                     verbose=2,save_best_only=True,\n",
    "                     save_weights_only=False,mode=\"auto\",save_freq=\"epoch\")\n",
    "\n",
    "# Fit the model with 100 epochs\n",
    "history = model.fit(Xtrain, ytrain, epochs = 100, verbose = 2, callbacks = [es,mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Summary**\n",
    "\n",
    "Lastly, we can take a look at the summary of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional multiple                  543744    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri multiple                  385       \n",
      "=================================================================\n",
      "Total params: 544,129\n",
      "Trainable params: 544,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load models\n",
    "model20d = keras.models.load_model('4-models/20 Day Lag.h5')\n",
    "\n",
    "model20d.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final model has close to 856 thousands parameters. The model also stopped training after 60 epochs, stopped by our early stopping mechanism. In the end, the weights with a Root Mean Squared Error of 38.60 was saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Next Step**\n",
    "\n",
    "In this notebook, we preprocessed the '20-Day Lag' dataset by scaling and reshaping to fit into our Bidirectional Long Short-Term Memory Recurrent Neural Network. We trained the model as a Weekly Predictor, and to prevent overfitting, we also called for an early stopping. In the end, our model as 544,129 parameters and achieved a Root Mean Squared Error of 38.60.\n",
    "\n",
    "At this point, however, we do not know how well our model will actually perform on the test set, which is 2019-08-18 and beyond. We will actually test the model with the other two models all in one single notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
