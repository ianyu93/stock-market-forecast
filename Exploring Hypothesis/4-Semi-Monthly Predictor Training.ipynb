{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Semi-Monthly Predictor Training**\n",
    "## December 2020\n",
    "### Ian Yu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Table of Content**\n",
    "\n",
    "1. [Objective](#Objective)\n",
    "2. [Setup](#Setup)\n",
    "3. [Preprocessing](#Preprocessing)\n",
    "4. [Model Building](#Model-Building)\n",
    "5. [Model Training](#Model-Training)\n",
    "6. [Model Summary](#Model-Summary)\n",
    "6. [Next Step](#Next-Step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Objective**\n",
    "\n",
    "The purpose of this notebook is to take the parameters provided in the previous notebook, '3-Hypertuning with 10-Day Engineered Dataframe', and train the model. We once again split the model training into 3 different notebooks as we perform the training process simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setup**\n",
    "\n",
    "As discussed in the previous notebook, we will be training a Bidirectional Long Short-Term Memory Recurrent Neural Network. Unlike the previous notebook, however, since we already performed hypertuning, we will just be training the entire train set, which is all dates before 2019-08-18, and find the best model based on lowest loss value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing boto3 to call object from AWS S3 personal bucket\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Object('capstone-ianyu', '2-10dengineered_df.csv').download_file('data/2-10dengineered_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing basic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Read the clean dataset\n",
    "df = pd.read_csv(\"data/2-10dengineered_df.csv\", index_col = 0)\n",
    "\n",
    "# To ensure the index is set at datetime\n",
    "df.index = pd.to_datetime(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preprocessing**\n",
    "\n",
    "We're going to split the train test and test set based on the notion that we will make prediction continouously for 250 days. After splitting, we will define the target, `SPX Today`, and features for both the train set and the test set. We will also be using MinMaxScaler, a more common scaler for time series problems, to scale our features in train set and test set. Note that MinMaxScaler would scale the features so that different features would stay on the same scale. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test Split, continously predicting for 250 days\n",
    "train = df[:-250]\n",
    "test = df[-250:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting features and target for train set\n",
    "X_train = train.drop('SPX Today', axis = 1)\n",
    "y_train = train['SPX Today']\n",
    "\n",
    "# Setting features and target for test set\n",
    "X_test = test.drop('SPX Today', axis = 1)\n",
    "y_test = test['SPX Today']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting MinMax Scaler onto the remainder set\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mmscaler = MinMaxScaler()\n",
    "\n",
    "# Converting to Dataframe to keep the Timestamps\n",
    "X_train_mms = pd.DataFrame(data = mmscaler.fit_transform(X_train), columns = X_train.columns, index = train.index)\n",
    "X_test_mms = pd.DataFrame(data = mmscaler.transform(X_test), columns = X_test.columns, index = test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping features and target in order to fit through our model\n",
    "Xtrain = np.array(X_train_mms).reshape(np.array(X_train_mms).shape[0],-1,X_train_mms.shape[1])\n",
    "ytrain = np.array(y_train).reshape(np.array(y_train).shape[0],-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Building**\n",
    "\n",
    "The architecture that we will be building is comprised of four elements. It will be a single layer of Bidirectional Long Short-Term Memory Recurrent Neural Network with an output layer of Time Distributed Layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recurrent Neural Netowrk** is a type of neural network that allows us to learn backwards in sequence. It takes in a 3D shape array, input the data as (batch size, sequence, features). Batch size would be the number of data points that we are passing into the neural network at once. If we have a batch size of 1024, then we are passing 1024 trading days into the network at a time. We will have the hypertuner to determine what is the optimal batch size. Sequence length is about the past context, where if we set a sequence length of 10, the model would learn how does the previous 10 sequence affect the current input. We will also leave it to the hypertuner to determine the best sequence length parameter. Features dimension would simply be the number of features we have in our dataset.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Recurrent_neural_network_unfold.svg/2880px-Recurrent_neural_network_unfold.svg.png\" height=800 width=800>\n",
    "\n",
    "*image from [Wikepedia](https://en.wikipedia.org/wiki/Recurrent_neural_network#/media/File:Recurrent_neural_network_unfold.svg)*\n",
    "\n",
    "**Long Short-Term Memory** is a special type of RNN that learns about the long term default behaviour of the dataset. In effect, this would decompose seasonality, trends, and other potential long-term patterns. \n",
    "\n",
    "**Bidirectional** is applied to the LSTM RNN for the purpose of learning the future context as well. Not only does the past affect the stock market today, but also the anticipation of tomorrow's environment would affect today's market. Therefore, we would also need to understand how that anticipation affects today's price. The bidirectional element creates a separate network in the same training session that learns forwards in the sequence instead, so that each time the network is learning both backwards and forward in time. \n",
    "\n",
    "**Time Distributed Layer** is a special type of output layer that keeps the training input and output one at a time, keeping the timestamps true. Without the layer, the default behaviour of RNN would learn and output in batches instead. \n",
    "\n",
    "*Note 1*: The inclusion of both Bidirectional element and the Time Distributed Layer was inspired by [Solving Sequence Problems with LSTM in Keras: Part 2](https://stackabuse.com/solving-sequence-problems-with-lstm-in-keras-part-2/). \n",
    "\n",
    "*Note 2*: This architecture is the final result of many trials and errors through monitoring the loss and train/validation metrics. Experiments include trying much more complicated structure, such as including RepeatVector mentioned in the tutorial from *Note 1*. We did not find more complex layers performed any better than the current architecture. Part of the decision was also influenced by [Optimizing LSTM for time series prediction in Indian stock market](https://pdf.sciencedirectassets.com/280203/1-s2.0-S1877050920X00056/1-s2.0-S1877050920307237/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjECQaCXVzLWVhc3QtMSJIMEYCIQCaPKGIG7DdWGcWijUd%2BmhnqxFo0lkvbaeANVEVr4Gx8AIhAP%2Fxavemri255BxSxSlvzXZIJ1rP2SZ7no2NS4uWTJgDKr0DCK3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQAxoMMDU5MDAzNTQ2ODY1IgzpgbluX2qDaLCDNLoqkQOaVtZa17kPj7Fik6ElKg8ZfyV8AGB6Gh3QEEvPhW655oNfCFFaVA2hzgs3ytWBA4e7AaFcCQifbETiM8qSekN5YP02N6MiSD9EV5uuhV3RHTMM6I6De3WwITh6Vg2SwhhVFb%2Bdxf7XgXbK9g7%2Fok1ctc6MhHQ%2FJHzD7AwF7NTJm2mzV0H%2F4tenShk153gVsDqWnHLrW2wYjQg7LtBAy2H89G9gI7TK1WIECXkutt6Moxh1GO1nRQxGIryu5OGnHphCIdAz3oEj91X7loih1geTon9J5EB3HSNeb%2BM%2Fhk%2BfNCInWLgPBnOJdQrX9A9OpbVVcF%2BvacbiPmS4OyGd8%2BkHVLE2svGbkDyiKTokP0TPBCh9NB9S3Ovt%2BoJbx6yGzXdu1gvqREgnqYcezySe5BdxygSCLg1iVcJx%2FLTduJo%2BLLt7EYycHSg14SiVLAO%2F%2B2AQxtV3FfYZZqqAdcuAj6epR1pqyJ%2BF%2FWZp6xHlEXhR4tnscsDvDExku%2FEJI0%2B6DAIL1POcsxP%2Bb9FGqCPd5WlNTzDl35%2F%2BBTrqAS8O9L6e1THIH3VjiRI5bLGQ%2FRNXXoc9oNfpqCOVjANdDrSE2tThA9V%2BNKd14Qb57yciOn93apfbjYvYdovXogLJ0cBBx3N%2FI6y2Z6ZsW0aIEKHx%2Bz6T3e6G5aXLafqMP%2B4JBiUYx0KmS8n8A%2BbbswEqYtbHkGFJxbKSjEzFbT8%2FLaFVD9mQMwZLs55UYmrohOysGZk%2BWnJjtsFj9W9TjYCLIpsXIGQBHABPNbPsDbQ9%2BJYk%2F3lY3xRmA060j3%2F0lHqQf%2Fy3svPUe7xhtCNAMo%2FSWMoxgdWJTmW0wE1MLmRbSYSMJrAqZN4pZg%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20201202T210705Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY3ADR5CWC%2F20201202%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=8fe53ed1da02e8720b05dbd4c47599a135d812d489b217c1e155ea7f8eba50c3&hash=f41fa7bc182cb22c436f1de61016bf5ecaa6158ccab1236efb2e593a3f997dd4&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1877050920307237&tid=spdf-af325d5e-2008-49c1-9511-03d086ba6774&sid=a182baa1112258464a79b52-2c38f3e10bdfgxrqa&type=client), finding more LSTM layers does not increase performance, but does stabalize the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a nutshell, our model architecture looks like this:\n",
    "\n",
    "<img src=\"images/ModelArchitecture.png\" height=800 width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing tensorflow and keras pakcages needed\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Bidirectional,TimeDistributed\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be building the model based on the configuration from the previous notebook, which would be:\n",
    "\n",
    "|Parameters|Configuration|\n",
    "|:---|:---|\n",
    "|**LSTM Units**|256|\n",
    "|**Sequence Length**|90|\n",
    "|**Initial Learning Rate**|0.01|\n",
    "|**Decay Steps**|10,000|\n",
    "|**Clipnorm**|1.0|\n",
    "\n",
    "We will also be using Mean Squared Error as our loss function and Root Mean Squared Error as our human readable metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learning rate schedule code is from Tensorflow documentation:\n",
    "## https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9, staircase = True)\n",
    "\n",
    "\n",
    "## Modeling with sequential\n",
    "model = keras.Sequential()\n",
    "\n",
    "## Building Bidrectional LSTM RNN\n",
    "# Configuration from previous notebook\n",
    "model.add(Bidirectional(LSTM(units = 256, activation='relu',\n",
    "                             input_shape = (90,X_train_mms.shape[1]),\n",
    "                             return_sequences=True)))\n",
    "\n",
    "# Time Distributed layer to keep the model time series\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "## Compiling model with Adam optimizer\n",
    "model.compile(optimizer = keras.optimizers.Adam(clipnorm=1.0, learning_rate = lr_schedule),\n",
    "              loss = 'mse', metrics = 'RootMeanSquaredError')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Training**\n",
    "\n",
    "Now that we have built the architecture of the model, we can start training. However, we cannot just blindly start training, especially when we do not have a validation dataset to check if we are overfitting. Therefore, we need to define an Early Stopping mechanism to prevent overfitting. We will be monitoring the loss function, and if the loss value is not improving for five times in a row, then the training will automatically stop. \n",
    "\n",
    "We will also call for a Model Checkpoint. Usually the purpose of the model check point is to find the check point with the best validation metrics performance, but here we are also using the Model Checkpoint to save the best performing model as '10 Day Lag.h5'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: root_mean_squared_error improved from inf to 319.53433, saving model to /Users/ianyu/capstone/4-models/10 Day Lag.h5\n",
      "271/271 - 2s - loss: 102102.1875 - root_mean_squared_error: 319.5343\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: root_mean_squared_error improved from 319.53433 to 53.71342, saving model to /Users/ianyu/capstone/4-models/10 Day Lag.h5\n",
      "271/271 - 2s - loss: 2885.1311 - root_mean_squared_error: 53.7134\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: root_mean_squared_error improved from 53.71342 to 49.82032, saving model to /Users/ianyu/capstone/4-models/10 Day Lag.h5\n",
      "271/271 - 2s - loss: 2482.0647 - root_mean_squared_error: 49.8203\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: root_mean_squared_error improved from 49.82032 to 45.37187, saving model to /Users/ianyu/capstone/4-models/10 Day Lag.h5\n",
      "271/271 - 2s - loss: 2058.6067 - root_mean_squared_error: 45.3719\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: root_mean_squared_error improved from 45.37187 to 44.67081, saving model to /Users/ianyu/capstone/4-models/10 Day Lag.h5\n",
      "271/271 - 2s - loss: 1995.4811 - root_mean_squared_error: 44.6708\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: root_mean_squared_error improved from 44.67081 to 44.50546, saving model to /Users/ianyu/capstone/4-models/10 Day Lag.h5\n",
      "271/271 - 2s - loss: 1980.7358 - root_mean_squared_error: 44.5055\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: root_mean_squared_error improved from 44.50546 to 43.73186, saving model to /Users/ianyu/capstone/4-models/10 Day Lag.h5\n",
      "271/271 - 2s - loss: 1912.4753 - root_mean_squared_error: 43.7319\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: root_mean_squared_error did not improve from 43.73186\n",
      "271/271 - 2s - loss: 1963.7095 - root_mean_squared_error: 44.3138\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: root_mean_squared_error did not improve from 43.73186\n",
      "271/271 - 2s - loss: 1973.1154 - root_mean_squared_error: 44.4198\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: root_mean_squared_error improved from 43.73186 to 42.91041, saving model to /Users/ianyu/capstone/4-models/10 Day Lag.h5\n",
      "271/271 - 2s - loss: 1841.3032 - root_mean_squared_error: 42.9104\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: root_mean_squared_error improved from 42.91041 to 41.32148, saving model to /Users/ianyu/capstone/4-models/10 Day Lag.h5\n",
      "271/271 - 2s - loss: 1707.4642 - root_mean_squared_error: 41.3215\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: root_mean_squared_error did not improve from 41.32148\n",
      "271/271 - 2s - loss: 1746.9424 - root_mean_squared_error: 41.7964\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: root_mean_squared_error did not improve from 41.32148\n",
      "271/271 - 2s - loss: 1707.5054 - root_mean_squared_error: 41.3220\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: root_mean_squared_error improved from 41.32148 to 40.21155, saving model to /Users/ianyu/capstone/4-models/10 Day Lag.h5\n",
      "271/271 - 2s - loss: 1616.9690 - root_mean_squared_error: 40.2116\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: root_mean_squared_error improved from 40.21155 to 39.83285, saving model to /Users/ianyu/capstone/4-models/10 Day Lag.h5\n",
      "271/271 - 2s - loss: 1586.6560 - root_mean_squared_error: 39.8329\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: root_mean_squared_error improved from 39.83285 to 39.13071, saving model to /Users/ianyu/capstone/4-models/10 Day Lag.h5\n",
      "271/271 - 2s - loss: 1531.2129 - root_mean_squared_error: 39.1307\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: root_mean_squared_error did not improve from 39.13071\n",
      "271/271 - 2s - loss: 1616.7820 - root_mean_squared_error: 40.2092\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: root_mean_squared_error did not improve from 39.13071\n",
      "271/271 - 2s - loss: 1639.0129 - root_mean_squared_error: 40.4847\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: root_mean_squared_error did not improve from 39.13071\n",
      "271/271 - 2s - loss: 1705.2869 - root_mean_squared_error: 41.2951\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 00020: root_mean_squared_error did not improve from 39.13071\n",
      "271/271 - 2s - loss: 1568.8612 - root_mean_squared_error: 39.6089\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 00021: root_mean_squared_error did not improve from 39.13071\n",
      "271/271 - 2s - loss: 1609.5793 - root_mean_squared_error: 40.1196\n"
     ]
    }
   ],
   "source": [
    "# Define early stopping, monitoring loss function, stops if not improving for five times in a row\n",
    "es = EarlyStopping(monitor='loss', patience = 5, verbose=0, mode='auto')\n",
    "\n",
    "# Define model checkpoint to save the best model weights and to be used immediately\n",
    "mc = ModelCheckpoint(\"/Users/ianyu/capstone/4-models/20 Day Lag.h5\",\n",
    "                     monitor=\"root_mean_squared_error\",\n",
    "                     verbose=2,save_best_only=True,\n",
    "                     save_weights_only=False,mode=\"auto\",save_freq=\"epoch\")\n",
    "\n",
    "# Fit the model with 100 epochs\n",
    "history = model.fit(Xtrain, ytrain, epochs = 100, verbose = 2, callbacks = [es,mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Summary**\n",
    "\n",
    "Lastly, we can take a look at the summary of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional multiple                  856064    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri multiple                  513       \n",
      "=================================================================\n",
      "Total params: 856,577\n",
      "Trainable params: 856,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load models\n",
    "model10d = keras.models.load_model('4-models/10 Day Lag.h5')\n",
    "\n",
    "model10d.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final model has close to 856 thousands parameters. The model also stopped training after 21 epochs, stopped by our early stopping mechanism. In the end, the weights with a Root Mean Squared Error of 39.13 was saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Next Step**\n",
    "\n",
    "In this notebook, we preprocessed the '10-Day Lag' dataset by scaling and reshaping to fit into our Bidirectional Long Short-Term Memory Recurrent Neural Network. We trained the model as a Weekly Predictor, and to prevent overfitting, we also called for an early stopping. In the end, our model as 856,577 parameters and achieved a Root Mean Squared Error of 39.13.\n",
    "\n",
    "At this point, however, we do not know how well our model will actually perform on the test set, which is 2019-08-18 and beyond. We will actually test the model with the other two models all in one single notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
