{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Cleaning**\n",
    "## December 2020\n",
    "### Ian Yu\n",
    "\n",
    "but on this one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Table of Content**\n",
    "\n",
    "1. [Objective](#Objective)\n",
    "2. [Acknoledgement](#Acknoledgement)\n",
    "3. [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "4. [Data Cleaning and Concatenating Dataframes](#Data-Cleaning-and-Concatenating-Dataframes)\n",
    "5. [Next Step](#Next-Step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Objective**\n",
    "The purpose of this notebook is to read the original datasets acquired, perform Exploratory Data Analysis(EDA), and export a workable dataframe for feature engineering and modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Acknoledgement**\n",
    "This project consists of 6 datasets acquired through various open data sources to ensure reproducibility and ease of validation. \n",
    "\n",
    "### S&P 500 Index: Proxy to Stock Market\n",
    "The S&P 500 Index dataset is downloaded throuhg [Yahoo! Finance](https://finance.yahoo.com/quote/%5EGSPC/history?p=%5EGSPC). S&P 500 Index is a stock index composed by Standard and Poor's to measure the stock performance of 500 large companies listed on stock exchanges in the United States. It is one of the most followed stock index in the world, and is widely regarded as the best single gauge of large-cap U.S. equities. The dataset consists of daily data of the data. The index is a free-float capitalization-weighted index, that is, companies are weighted in the index in proportion to their market capitalizations. \n",
    "\n",
    "`Date`: Date of the data point, 1973-01-02 to 2020-11-17 in daily format. The market usually opens from Monday to Friday, except on major holidays.\n",
    "\n",
    "`Open`: Index value when the market opens. Note that `Open` is not, though usually quite close, necessarily the `Close` of previous day, as off-market hours trades also affect its values.\n",
    "\n",
    "`High`: The highest point of index value at a given day, typically not equivalent to `Open` nor `Close`. The highest point typically indicates where the resistence is.\n",
    "\n",
    "`Low`: The Lowest point of index value at a given day, typically not equivalent to `Open` nor `Close`. The lowest point typically indicates where the support is.\n",
    "\n",
    "`Close`: The closing, or final, index value of the market on a given day, also the value when people refer to the index value on any given day. \n",
    "\n",
    "`Adj Close`: Adjusted close is adjusted for both dividends and splits. In other words, it is accounting for additional corporate actions that may affect the market, typically used for qualitative research on corporate accounting level. For more detailed explanation on the differences between close and adjusted close can be read [here](https://www.investopedia.com/terms/a/adjusted_closing_price.asp).This does not apply to the index itself, however, and as a result contains the same values as `Close`\n",
    "\n",
    "`Volume`: Trading volume is the total number of shares traded on a given day. This represents the magnitude of trading activity. One could have a drastic change in index value despite lower trading volume, so it is always good to check for trading volume to guage the nature of value changes.\n",
    "\n",
    "### 10Y US Government Bond Yields: Proxy to Bond Market\n",
    "The dataset comes from [DataHub](https://datahub.io/core/bond-yields-us-10y). The US government bonds are generally seen as representative of quality fixed income asset. 10Y Government Bond is chosen for it sits right between extremely long-term fixed assets (30Y Bonds) and short-term investment parking (3 months ~ 2 years). Bond yield is the return an investor realizes on a bond. For more details on the bond yield, read [here](https://www.investopedia.com/terms/b/bond-yield.asp).\n",
    "\n",
    "`Date`: Date of the data point, from 1953-04-30 to 2020-06-30, monthly data. \n",
    "\n",
    "`Rate`: 10 year nominal yields on US government bonds from the Federal Reserve. The 10 year government bond yield is considered a standard indicator of long-term interest rates. \n",
    "\n",
    "### Dollar Index or DXY: Proxy to Currency Market\n",
    "\n",
    "The dataset also comes from [Yahoo! Finance](https://finance.yahoo.com/quote/DX-Y.NYB/history?period1=94694400&period2=1606348800&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true). Because currency is always trades against another, the exchange rate is always relative. The Dollar Index is a trade-weighted index of the dollar's strength against a basket of US's most significant trading partners. Although the trading weight of the index has not been updated since the creation of the Euro, thus seen obsolete sometimes, it is still the most followed indicator regarding to the dollar's strength.\n",
    "\n",
    "`Date`: Date of the data point, 1973-01-02 to 2020-11-17 in daily format. The market usually opens from Monday to Friday, except on major holidays.\n",
    "\n",
    "`Open`: Index value when the market opens. Note that `Open` is not, though usually quite close, necessarily the `Close` of previous day, as off-market hours trades also affect its values.\n",
    "\n",
    "`High`: The highest point of index value at a given day, typically not equivalent to `Open` nor `Close`. The highest point typically indicates where the resistence is.\n",
    "\n",
    "`Low`: The Lowest point of index value at a given day, typically not equivalent to `Open` nor `Close`. The lowest point typically indicates where the support is.\n",
    "\n",
    "`Close`: The closing, or final, index value of the market on a given day, also the value when people refer to the index value on any given day. The close price is adjusted for stock splits, as stock splits dramatically changes the price and index-weighting of the stock.\n",
    "\n",
    "`Adj Close`: As with S&P 500, the `Adj Close` values would be identical to `Close` and thus not useful in our project.\n",
    "\n",
    "### WTI Spot Price: Proxy to Commodity\n",
    "\n",
    "West Texas Intermediate oil spot price dataset from [DataHub](https://datahub.io/JohnSnowLabs/brent-and-wti-monthly-spot-prices). WTI historically has been used to track US oil. Although now it has gained global adoptoin and does not discriminate origin, it is still more relavent to the US financial market than other oil index. We are using spot price, meaning the price on the day, instead of futures. While the commodity market is enormous, including cocoa, sugar, and metals, the oil market is one of the most representative and influential commodity in the market. \n",
    "\n",
    "`Date`: 1986-01-03 to 2020-08-28 in weekly format. The market usually opens from Monday to Friday, except on major holidays. \n",
    "\n",
    "`Price`: Closing price on Fridays. \n",
    "\n",
    "### GOLD: Proxy to Commodity-ish\n",
    "\n",
    "Gold dataset from [Datahub](https://datahub.io/core/gold-prices#data). While gold is a precious metal commodity, it also has a unique status of reserve currency, making it a hedge against bad times. When the market is pessimistic, money often flows from the stock market to gold. Gold is very representative in cross-market sentiment.\n",
    "\n",
    "`Date`: 1950-01 to 2020-07, in monthly format. While the market opens on normal trading days, this is one of the few open data that spans beyond 2000. \n",
    "\n",
    "`Price`: Closing price of the month in USD. \n",
    "\n",
    "### CPI: Proxy to Inflation Environment\n",
    "\n",
    "One of the most important underlying concept of Intermarket Analysis is that the inflation environment also affects how each of the four major markets is correlated with one another. For example, stocks and bonds used to have a positive correlation back in the 70s. Comparing to decades ago, we now are under what is considered a deflationary environment, where we are more worried about not having enough inflation and may even enter deflation. We source the US Consumer Price Index, one of the most prominent index to measure US inflation, from [Federal Reserve Economic Data](https://fred.stlouisfed.org/series/CPALTT01USA657N) of St. Louis Fed. As we are taking annual inflation rate to represent the long-term macro environment, FRED does not have inflation rate for 2020 yet. As a result, we are using a forecasted number by [Desjardins](https://www.desjardins.com/ressources/pdf/pefm2008-e.pdf?resVer=1597926090000) for our 2020 inflation rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploratory Data Analysis**\n",
    "\n",
    "During the exploratory data analysis, we will be checking for missing data and datatype. While we know that our datasets should be relatively clean and without missing data, a standard checkup is a required practice. Additionally, the datasets all come from different sources, but we need all of the data to be in one single dataset to start further Feature Engineering and Training. Rather than assuming the datasets are clean and simply concating them togther, we will need to examine the datasets individually to decide how to treat the datasets to align their format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the holy trinity of data science in python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv for all datasets in the data folder\n",
    "stock = pd.read_csv('data/0-SPX.csv')\n",
    "yields = pd.read_csv('data/0-10YNominalYields.csv')\n",
    "usd = pd.read_csv('data/0-DXY.csv')\n",
    "gold = pd.read_csv('data/0-GOLD_USD.csv')\n",
    "wti = pd.read_csv('data/0-WTI.csv')\n",
    "cpi = pd.read_csv('data/0-CPI_Y.csv')\n",
    "\n",
    "# Put the dataframes into a list for certain treat-all operations\n",
    "datalist = [stock, yields, usd, gold, wti, cpi]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look into the schema and datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12079 entries, 0 to 12078\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Date       12079 non-null  object \n",
      " 1   Open       12079 non-null  float64\n",
      " 2   High       12079 non-null  float64\n",
      " 3   Low        12079 non-null  float64\n",
      " 4   Close      12079 non-null  float64\n",
      " 5   Adj Close  12079 non-null  float64\n",
      " 6   Volume     12079 non-null  int64  \n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 660.7+ KB\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 808 entries, 0 to 807\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Date    808 non-null    object \n",
      " 1   Rate    808 non-null    float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 12.8+ KB\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14873 entries, 0 to 14872\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Date       14873 non-null  object \n",
      " 1   Open       12166 non-null  float64\n",
      " 2   High       12166 non-null  float64\n",
      " 3   Low        12166 non-null  float64\n",
      " 4   Close      12166 non-null  float64\n",
      " 5   Adj Close  12166 non-null  float64\n",
      " 6   Volume     12166 non-null  float64\n",
      "dtypes: float64(6), object(1)\n",
      "memory usage: 813.5+ KB\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 847 entries, 0 to 846\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Date    847 non-null    object \n",
      " 1   Price   847 non-null    float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 13.4+ KB\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1809 entries, 0 to 1808\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Date    1809 non-null   object \n",
      " 1   Price   1809 non-null   float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 28.4+ KB\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60 entries, 0 to 59\n",
      "Data columns (total 2 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   DATE             60 non-null     object \n",
      " 1   CPALTT01USA657N  60 non-null     float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 1.1+ KB\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calling info for every dataframe in the datalist\n",
    "for df in datalist:\n",
    "    df.info()\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the schema and datatypes above, we can see that while the prices and volumes are in the right datatype, dates are not. We will have to make sure the dates are converted into datetime format later. Also, CPI's column names are capitalized, and the column name for inflation rate is not reader friendly. Keeping in mind of these, we continue to examine the shape and features for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape is (12079, 7), has 0 missing values, and 0 duplicated values\n",
      "shape is (808, 2), has 0 missing values, and 0 duplicated values\n",
      "shape is (14873, 7), has 16242 missing values, and 0 duplicated values\n",
      "shape is (847, 2), has 0 missing values, and 0 duplicated values\n",
      "shape is (1809, 2), has 0 missing values, and 0 duplicated values\n",
      "shape is (60, 2), has 0 missing values, and 0 duplicated values\n"
     ]
    }
   ],
   "source": [
    "# Checking if there is any missing values in each of the dataframe\n",
    "\n",
    "for df in datalist:\n",
    "    print(f\"shape is {df.shape}, has {df.isna().sum().sum()} missing values, and {df.duplicated().sum().sum()} duplicated values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we are not working with any duplicated values, we can see that the third item in `datalist`, which is `usd`, actually has many missing values. We also see that the number of rows vary widely. This is expected, as we are working not just daily data, but also weekly, monthly, and even annual data. We next have to check why does `usd` have missing values. In treating missing values, we have to understand if it's Missing At Random, Missing Completely At Random, Missing Not At Random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date            0\n",
       "Open         2707\n",
       "High         2707\n",
       "Low          2707\n",
       "Close        2707\n",
       "Adj Close    2707\n",
       "Volume       2707\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many missing values are there in each column of usd dataframe?\n",
    "usd.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are exactly 2707 missing values in every column except `Date`. So there is some patterns into this, perhaps not missing by random. We will now return rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1973-01-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1973-01-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1973-01-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1973-01-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1973-01-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14845</th>\n",
       "      <td>2020-10-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14851</th>\n",
       "      <td>2020-11-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14857</th>\n",
       "      <td>2020-11-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14863</th>\n",
       "      <td>2020-11-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14869</th>\n",
       "      <td>2020-11-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2707 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date  Open  High  Low  Close  Adj Close  Volume\n",
       "4      1973-01-07   NaN   NaN  NaN    NaN        NaN     NaN\n",
       "10     1973-01-14   NaN   NaN  NaN    NaN        NaN     NaN\n",
       "16     1973-01-21   NaN   NaN  NaN    NaN        NaN     NaN\n",
       "20     1973-01-25   NaN   NaN  NaN    NaN        NaN     NaN\n",
       "22     1973-01-28   NaN   NaN  NaN    NaN        NaN     NaN\n",
       "...           ...   ...   ...  ...    ...        ...     ...\n",
       "14845  2020-10-25   NaN   NaN  NaN    NaN        NaN     NaN\n",
       "14851  2020-11-01   NaN   NaN  NaN    NaN        NaN     NaN\n",
       "14857  2020-11-08   NaN   NaN  NaN    NaN        NaN     NaN\n",
       "14863  2020-11-15   NaN   NaN  NaN    NaN        NaN     NaN\n",
       "14869  2020-11-22   NaN   NaN  NaN    NaN        NaN     NaN\n",
       "\n",
       "[2707 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return where Open has missing values.\n",
    "usd[usd['Open'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that where there are missing values, all the other columns also have missing values. We also find a 7-day jump in each of the rows. A quick check into the calendar, we quickly realize it's Sundays. So our `usd` data actually includes non-trading day. We will have to keep that in mind going forward. Before we decide how to treat these missing values, we still want to further examine other datasets. Our next step is to take a look at the values within each of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1973-01-02</td>\n",
       "      <td>118.059998</td>\n",
       "      <td>119.900002</td>\n",
       "      <td>118.059998</td>\n",
       "      <td>119.099998</td>\n",
       "      <td>119.099998</td>\n",
       "      <td>17090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1973-01-03</td>\n",
       "      <td>119.099998</td>\n",
       "      <td>120.449997</td>\n",
       "      <td>118.690002</td>\n",
       "      <td>119.570000</td>\n",
       "      <td>119.570000</td>\n",
       "      <td>20620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1973-01-04</td>\n",
       "      <td>119.570000</td>\n",
       "      <td>120.169998</td>\n",
       "      <td>118.120003</td>\n",
       "      <td>119.400002</td>\n",
       "      <td>119.400002</td>\n",
       "      <td>20230000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        Open        High         Low       Close   Adj Close  \\\n",
       "0  1973-01-02  118.059998  119.900002  118.059998  119.099998  119.099998   \n",
       "1  1973-01-03  119.099998  120.449997  118.690002  119.570000  119.570000   \n",
       "2  1973-01-04  119.570000  120.169998  118.120003  119.400002  119.400002   \n",
       "\n",
       "     Volume  \n",
       "0  17090000  \n",
       "1  20620000  \n",
       "2  20230000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1953-04-30</td>\n",
       "      <td>2.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1953-05-31</td>\n",
       "      <td>3.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1953-06-30</td>\n",
       "      <td>3.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Rate\n",
       "0  1953-04-30  2.83\n",
       "1  1953-05-31  3.05\n",
       "2  1953-06-30  3.11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1973-01-02</td>\n",
       "      <td>110.199997</td>\n",
       "      <td>110.199997</td>\n",
       "      <td>110.199997</td>\n",
       "      <td>110.199997</td>\n",
       "      <td>110.199997</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1973-01-03</td>\n",
       "      <td>110.250000</td>\n",
       "      <td>110.250000</td>\n",
       "      <td>110.250000</td>\n",
       "      <td>110.250000</td>\n",
       "      <td>110.250000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1973-01-04</td>\n",
       "      <td>110.250000</td>\n",
       "      <td>110.250000</td>\n",
       "      <td>110.250000</td>\n",
       "      <td>110.250000</td>\n",
       "      <td>110.250000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        Open        High         Low       Close   Adj Close  \\\n",
       "0  1973-01-02  110.199997  110.199997  110.199997  110.199997  110.199997   \n",
       "1  1973-01-03  110.250000  110.250000  110.250000  110.250000  110.250000   \n",
       "2  1973-01-04  110.250000  110.250000  110.250000  110.250000  110.250000   \n",
       "\n",
       "   Volume  \n",
       "0     0.0  \n",
       "1     0.0  \n",
       "2     0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1950-01</td>\n",
       "      <td>34.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1950-02</td>\n",
       "      <td>34.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1950-03</td>\n",
       "      <td>34.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Date  Price\n",
       "0  1950-01  34.73\n",
       "1  1950-02  34.73\n",
       "2  1950-03  34.73"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1986-01-03</td>\n",
       "      <td>25.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1986-01-10</td>\n",
       "      <td>25.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1986-01-17</td>\n",
       "      <td>24.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Price\n",
       "0  1986-01-03  25.78\n",
       "1  1986-01-10  25.99\n",
       "2  1986-01-17  24.57"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>CPALTT01USA657N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1960-01-01</td>\n",
       "      <td>1.457976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1961-01-01</td>\n",
       "      <td>1.070724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1962-01-01</td>\n",
       "      <td>1.198773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DATE  CPALTT01USA657N\n",
       "0  1960-01-01         1.457976\n",
       "1  1961-01-01         1.070724\n",
       "2  1962-01-01         1.198773"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for value in each dataframe in datalist\n",
    "for df in datalist:\n",
    "    display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12164"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## usd dataframe value has 0.0 as its values. Is that true for all the rows?\n",
    "# Return number of rows where usd['Volume'] equals to 0.0\n",
    "usd[usd['Volume']==0.0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few things that are important to change. First of all, as mentioned before, we see that the last dataframe, `cpi`, actually has an all capitalized column name rather than 'Date' like other dataframes. Since we are going to concat the dataframes on 'Date' as our common key, we will need to make sure the column name matches. While changing the column name for `cpi` dataframe, we will also make 'CPALTT01USA657N', which is the code on Federal Reserve Economic Data, more readable by changing it to 'Annual Rate' instead. \n",
    "\n",
    "We also seee `Adj Close` is indentical to `Close` in both the `stock` dataframe and `usd` dataframe. We can drop these two columns. We also see that `Volume` in `usd` dataframe basically is all zeros, so we will also have to drop this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicated columns and where with zero values only \n",
    "usd.drop(columns = ['Volume','Adj Close'], inplace = True)\n",
    "stock.drop(columns = ['Adj Close'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the column name from DATE to Date for cpi, CPALTT01USA657N to Annual Rate\n",
    "cpi.rename(columns = {'DATE': 'Date', 'CPALTT01USA657N': 'Annual Rate'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After treating the column names, we are going to concatenate the dataframes, but as noted before, the dates are all over the place. We will have to find out the latest starting date and the earliest ending date across the dataframes to decide a cut-off for our main dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earliest dates are 1973-01-02 00:00:00\n",
      "latest dates are 2020-11-18 00:00:00\n",
      "\n",
      " \n",
      "earliest dates are 1953-04-30 00:00:00\n",
      "latest dates are 2020-07-31 00:00:00\n",
      "\n",
      " \n",
      "earliest dates are 1973-01-02 00:00:00\n",
      "latest dates are 2020-11-25 00:00:00\n",
      "\n",
      " \n",
      "earliest dates are 1950-01-01 00:00:00\n",
      "latest dates are 2020-07-01 00:00:00\n",
      "\n",
      " \n",
      "earliest dates are 1986-01-03 00:00:00\n",
      "latest dates are 2020-08-28 00:00:00\n",
      "\n",
      " \n",
      "earliest dates are 1960-01-01 00:00:00\n",
      "latest dates are 2019-01-01 00:00:00\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Decide the date range for the entire dataset\n",
    "for df in datalist:\n",
    "    # Cast date to datetime\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # Check for the earliest and latest dates\n",
    "    print(f\"earliest dates are {df.iloc[:,0].min()}\")\n",
    "    print(f\"latest dates are {df.iloc[:,0].max()}\")\n",
    "    print('\\n ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the latest start date is 1986-01-03 and the earliest end date is 2020-07-31, so we will make those two as our starting date and ending date for the final merged dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Cleaning and Concatenating Dataframes**\n",
    "\n",
    "As we are going to perform major operations onto our dataframes, we will cautiously create a copy for each of the dataframe and work within those copies instead of the original dataframes themselves. This way, if anything goes wrong for our next few operations, we do not have to restart the kernel again. \n",
    "\n",
    "As we are performing time series forecast, we will also be setting `Date` as our index. This will be helpful when we are performing train-test split in later notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copied dataframe to preserve the original data while working with the new dataframe\n",
    "spx_df = stock.copy()\n",
    "us10y_df = yields.copy()\n",
    "dxy_df = usd.copy()\n",
    "wti_df = wti.copy()\n",
    "gold_df = gold.copy()\n",
    "cpi_df = cpi.copy()\n",
    "\n",
    "# Create a new iterable for the copied dataframes\n",
    "df_list = [spx_df, us10y_df, dxy_df, wti_df, gold_df, cpi_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all the date columns into datetime, and preserve everything between 1986-01-03 and 2020-07-31\n",
    "for df in df_list:\n",
    "    \n",
    "    # Dropping every row outside of date range for each dataframe\n",
    "    df.drop(df[(df['Date'] < '1986-01-01') | (df['Date']>'2020-07-31')].index, inplace = True)\n",
    "    \n",
    "    # Set dates to index\n",
    "    df.set_index(keys = 'Date',inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we concatenate the dataframes, we also need to make sure they have the same number of rows. That means all the dataframes have to be in daily format rather than weekly, monthly, or annual. We will expand the index with `reindex` method and contain only business days. By using `reindex` with a specified date range, we ensure the time series index for every dataframe is the same. We would, however, be introducing new missing values. For annual, monthly and weekly dataframes, as we are expanding into daily dataframes, there are going to be many days that do not have a value of. For the daily dataframes, on the other hand, as reindexing with \"business days\" would actually miss out on statutory holidays landed on weekdays, we will still have the missing values on these statutory holiday weekdays. \n",
    "\n",
    "Regarding statutory holiday weekdays, `dxy_df` is also a special case here. As mentioned before, it seemed that the missing values were on Sundays only, but in data science, we also need to be thorough. While I'm not showing the messy work here, I did perform deeper dive into the missing values to find out that there are missing values on Satutory Holidays that landed on weekdays. Reindexing fill method would only fill newly introduced missing values, so unlike `spx_df` where it could reindex and fill the newly introduced missing values at the same time, `dxy_df`'s missing values has to be treated *after* reindexing.\n",
    "\n",
    "We also cannot just apply backfill or forwardfill blindly to each dataframe. Note that forwardfill is to bring the last valid data \"forward\" to fill the missing values, while backfill is to bring the next valid data \"backwards\" to fill the missing values. That means for the non-daily dataframes, whether the original data landed on the beginning of the period or the end of the period matters. For example `us10y_df` has its values on the end of month to represent the value of that month (e.g. 2020-01-31), but `gold_df` has its values on the first day of the month (e.g. 2020-01-01). We will use backfill for dataframes that has values at the end of the month, and use forwardfill for dataframes that has its values at the beginning of a period. This is also why we did not merge the dataframes blindly together before performing this operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a date range of our interest, freq = 'B' for business days\n",
    "date_range = pd.date_range(start = '1986-01-03', end = '2020-07-31', freq = 'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Expand the index to daily frequency\n",
    "## Writing lines of codes instead because we're performing inplace; reindex does copies only\n",
    "## ffill where price of the period is at the beginning of the period, bfill where price of the period is at the end of the period\n",
    "\n",
    "# Expanding and ffill missing values introduced, i.e. holidays\n",
    "spx_df = spx_df.reindex(index = date_range, method = 'ffill')\n",
    "\n",
    "# Expanding and ffill missing values AFTER reindexing to treat both existing and newly introduced missing values\n",
    "dxy_df = dxy_df.reindex(index = date_range).fillna(method = 'ffill')\n",
    "\n",
    "# End of month, bfill to fill the month\n",
    "us10y_df = us10y_df.reindex(index = date_range, method = \"bfill\")\n",
    "\n",
    "# End of week, bfill to fill the week\n",
    "wti_df = wti_df.reindex(index = date_range, method = \"bfill\")\n",
    "\n",
    "# Start of Month, ffill to fill the week\n",
    "gold_df = gold_df.reindex(index = date_range, method = \"ffill\")\n",
    "\n",
    "# Start of Year, ffill to fill the year\n",
    "cpi_df = cpi_df.reindex(index = date_range, method = \"ffill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remember that `cpi_df` did not have 2020 inflation rate, and performing `ffill` method would actually fill 2019's inflation rate for all 2020 data points, which is definitely not representative of the year. We will refer to [Desjardins](https://www.desjardins.com/ressources/pdf/pefm2008-e.pdf?resVer=1597926090000) forecast for 2020 inflation rate, 1.1%, to fill the annual inflation rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Open     0\n",
       "High     0\n",
       "Low      0\n",
       "Close    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dxy_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 2020 annual inflation to 1.1\n",
    "cpi_df['2020-01-01':] = 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we actually concat the dataframes, we notice that the column names are actually similar, if not identical, so we will have to add a prefix to each of the column indicating the assets/market they are representing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a string on the column names to indicate the market for each dataframe\n",
    "spx_df.columns = \"SPX \" + spx_df.columns\n",
    "us10y_df.columns = \"US10Y \" + us10y_df.columns\n",
    "dxy_df.columns = \"DXY \" + dxy_df.columns\n",
    "wti_df.columns = \"WTI \" + wti_df.columns\n",
    "gold_df.columns = \"GOLD \" + gold_df.columns\n",
    "cpi_df.columns = \"CPI \" + cpi_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating all dataframes\n",
    "all_df = pd.concat([spx_df, us10y_df, dxy_df, wti_df, gold_df, cpi_df], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note that we did not fully treat the Statutory Holidays yet, and if we look at our `all_df`, we actually get 99 duplicated rows. Those rows are the statutory holidays that we filled the values of. In a typical structured learning problem, we would drop the duplicated values, but in this case, using `drop_duplicates` may actually drop real trading days. Since there are only 99 rows out of 9021 rows, we are not introducing significant bias. As there may be hidden patterns from Monday to Friday, or every five data points, we will keep the duplicated values to keep consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SPX Open</th>\n",
       "      <th>SPX High</th>\n",
       "      <th>SPX Low</th>\n",
       "      <th>SPX Close</th>\n",
       "      <th>SPX Volume</th>\n",
       "      <th>US10Y Rate</th>\n",
       "      <th>DXY Open</th>\n",
       "      <th>DXY High</th>\n",
       "      <th>DXY Low</th>\n",
       "      <th>DXY Close</th>\n",
       "      <th>WTI Price</th>\n",
       "      <th>GOLD Price</th>\n",
       "      <th>CPI Annual Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1986-03-28</th>\n",
       "      <td>237.300003</td>\n",
       "      <td>240.110001</td>\n",
       "      <td>237.300003</td>\n",
       "      <td>238.970001</td>\n",
       "      <td>178100000</td>\n",
       "      <td>7.78</td>\n",
       "      <td>117.220001</td>\n",
       "      <td>117.849998</td>\n",
       "      <td>117.220001</td>\n",
       "      <td>117.680000</td>\n",
       "      <td>12.00</td>\n",
       "      <td>346.095</td>\n",
       "      <td>1.898048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-07-04</th>\n",
       "      <td>252.699997</td>\n",
       "      <td>252.940002</td>\n",
       "      <td>251.229996</td>\n",
       "      <td>251.789993</td>\n",
       "      <td>108300000</td>\n",
       "      <td>7.30</td>\n",
       "      <td>111.349998</td>\n",
       "      <td>111.500000</td>\n",
       "      <td>111.010002</td>\n",
       "      <td>111.080002</td>\n",
       "      <td>12.23</td>\n",
       "      <td>348.554</td>\n",
       "      <td>1.898048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-11-27</th>\n",
       "      <td>248.139999</td>\n",
       "      <td>248.899994</td>\n",
       "      <td>247.729996</td>\n",
       "      <td>248.770004</td>\n",
       "      <td>152000000</td>\n",
       "      <td>7.25</td>\n",
       "      <td>106.529999</td>\n",
       "      <td>106.900002</td>\n",
       "      <td>106.379997</td>\n",
       "      <td>106.589996</td>\n",
       "      <td>15.01</td>\n",
       "      <td>396.983</td>\n",
       "      <td>1.898048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-12-25</th>\n",
       "      <td>246.339996</td>\n",
       "      <td>247.220001</td>\n",
       "      <td>246.020004</td>\n",
       "      <td>246.750000</td>\n",
       "      <td>95410000</td>\n",
       "      <td>7.11</td>\n",
       "      <td>105.610001</td>\n",
       "      <td>105.610001</td>\n",
       "      <td>105.379997</td>\n",
       "      <td>105.480003</td>\n",
       "      <td>17.05</td>\n",
       "      <td>391.595</td>\n",
       "      <td>1.898048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987-04-17</th>\n",
       "      <td>284.450012</td>\n",
       "      <td>289.570007</td>\n",
       "      <td>284.440002</td>\n",
       "      <td>286.910004</td>\n",
       "      <td>189600000</td>\n",
       "      <td>8.02</td>\n",
       "      <td>97.180000</td>\n",
       "      <td>97.320000</td>\n",
       "      <td>96.889999</td>\n",
       "      <td>96.949997</td>\n",
       "      <td>18.30</td>\n",
       "      <td>439.665</td>\n",
       "      <td>3.664563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-04</th>\n",
       "      <td>2978.080078</td>\n",
       "      <td>2995.840088</td>\n",
       "      <td>2977.959961</td>\n",
       "      <td>2995.820068</td>\n",
       "      <td>1963720000</td>\n",
       "      <td>2.06</td>\n",
       "      <td>96.809998</td>\n",
       "      <td>96.879997</td>\n",
       "      <td>96.599998</td>\n",
       "      <td>96.769997</td>\n",
       "      <td>57.32</td>\n",
       "      <td>1414.611</td>\n",
       "      <td>1.812210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-28</th>\n",
       "      <td>3145.489990</td>\n",
       "      <td>3154.260010</td>\n",
       "      <td>3143.409912</td>\n",
       "      <td>3153.629883</td>\n",
       "      <td>3033090000</td>\n",
       "      <td>1.81</td>\n",
       "      <td>98.300003</td>\n",
       "      <td>98.440002</td>\n",
       "      <td>98.279999</td>\n",
       "      <td>98.410004</td>\n",
       "      <td>58.07</td>\n",
       "      <td>1471.921</td>\n",
       "      <td>1.812210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-25</th>\n",
       "      <td>3225.449951</td>\n",
       "      <td>3226.429932</td>\n",
       "      <td>3220.510010</td>\n",
       "      <td>3223.379883</td>\n",
       "      <td>1296540000</td>\n",
       "      <td>1.86</td>\n",
       "      <td>97.669998</td>\n",
       "      <td>97.820000</td>\n",
       "      <td>97.580002</td>\n",
       "      <td>97.680000</td>\n",
       "      <td>61.29</td>\n",
       "      <td>1480.025</td>\n",
       "      <td>1.812210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-10</th>\n",
       "      <td>2776.989990</td>\n",
       "      <td>2818.570068</td>\n",
       "      <td>2762.360107</td>\n",
       "      <td>2789.820068</td>\n",
       "      <td>7880140000</td>\n",
       "      <td>0.66</td>\n",
       "      <td>100.129997</td>\n",
       "      <td>100.300003</td>\n",
       "      <td>99.360001</td>\n",
       "      <td>99.519997</td>\n",
       "      <td>24.41</td>\n",
       "      <td>1680.030</td>\n",
       "      <td>1.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-03</th>\n",
       "      <td>3143.639893</td>\n",
       "      <td>3165.810059</td>\n",
       "      <td>3124.520020</td>\n",
       "      <td>3130.010010</td>\n",
       "      <td>4190830000</td>\n",
       "      <td>0.62</td>\n",
       "      <td>97.150002</td>\n",
       "      <td>97.339996</td>\n",
       "      <td>96.809998</td>\n",
       "      <td>97.320000</td>\n",
       "      <td>39.85</td>\n",
       "      <td>1840.807</td>\n",
       "      <td>1.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               SPX Open     SPX High      SPX Low    SPX Close  SPX Volume  \\\n",
       "1986-03-28   237.300003   240.110001   237.300003   238.970001   178100000   \n",
       "1986-07-04   252.699997   252.940002   251.229996   251.789993   108300000   \n",
       "1986-11-27   248.139999   248.899994   247.729996   248.770004   152000000   \n",
       "1986-12-25   246.339996   247.220001   246.020004   246.750000    95410000   \n",
       "1987-04-17   284.450012   289.570007   284.440002   286.910004   189600000   \n",
       "...                 ...          ...          ...          ...         ...   \n",
       "2019-07-04  2978.080078  2995.840088  2977.959961  2995.820068  1963720000   \n",
       "2019-11-28  3145.489990  3154.260010  3143.409912  3153.629883  3033090000   \n",
       "2019-12-25  3225.449951  3226.429932  3220.510010  3223.379883  1296540000   \n",
       "2020-04-10  2776.989990  2818.570068  2762.360107  2789.820068  7880140000   \n",
       "2020-07-03  3143.639893  3165.810059  3124.520020  3130.010010  4190830000   \n",
       "\n",
       "            US10Y Rate    DXY Open    DXY High     DXY Low   DXY Close  \\\n",
       "1986-03-28        7.78  117.220001  117.849998  117.220001  117.680000   \n",
       "1986-07-04        7.30  111.349998  111.500000  111.010002  111.080002   \n",
       "1986-11-27        7.25  106.529999  106.900002  106.379997  106.589996   \n",
       "1986-12-25        7.11  105.610001  105.610001  105.379997  105.480003   \n",
       "1987-04-17        8.02   97.180000   97.320000   96.889999   96.949997   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2019-07-04        2.06   96.809998   96.879997   96.599998   96.769997   \n",
       "2019-11-28        1.81   98.300003   98.440002   98.279999   98.410004   \n",
       "2019-12-25        1.86   97.669998   97.820000   97.580002   97.680000   \n",
       "2020-04-10        0.66  100.129997  100.300003   99.360001   99.519997   \n",
       "2020-07-03        0.62   97.150002   97.339996   96.809998   97.320000   \n",
       "\n",
       "            WTI Price  GOLD Price  CPI Annual Rate  \n",
       "1986-03-28      12.00     346.095         1.898048  \n",
       "1986-07-04      12.23     348.554         1.898048  \n",
       "1986-11-27      15.01     396.983         1.898048  \n",
       "1986-12-25      17.05     391.595         1.898048  \n",
       "1987-04-17      18.30     439.665         3.664563  \n",
       "...               ...         ...              ...  \n",
       "2019-07-04      57.32    1414.611         1.812210  \n",
       "2019-11-28      58.07    1471.921         1.812210  \n",
       "2019-12-25      61.29    1480.025         1.812210  \n",
       "2020-04-10      24.41    1680.030         1.100000  \n",
       "2020-07-03      39.85    1840.807         1.100000  \n",
       "\n",
       "[99 rows x 13 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing the duplicated rows\n",
    "all_df[all_df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Next Step**\n",
    "\n",
    "In this notebook, we have explored all 6 datasets collected from public sources. We have treated missing values and misaligned datatypes. We also set the Dates as our index for the purpose of analyzing time series data and realign the timeline for all datasets. As we realigned the timeline, we also treated newly introduced missing values individually to ensure the final dataset is not interfered by weekends and statutory holidays on weekdays. Last but not least, we have added a prefix to each of the column, indicating what asset/market does each of the columnn represent before concatenation. \n",
    "\n",
    "Now that we have a single cleaned up dataset to work with, we will export it as `1-cleaned_df` (*note: 1 indicates step 1*). Our next step is to conduct Feature Engineering, which we will go through in details in the 2-Feature Engineering notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the dataframe as csv\n",
    "all_df.to_csv('data/1-cleaned_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}